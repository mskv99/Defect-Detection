# Defect-Detection

## Введение 
Область детектирования изображений с использованием глубокого обучения постоянно развивается – появляется множество новых техник и моделей, которые находят применение в промышленном производстве. Одной из таких возможных областей применения является детектирование дефектов в фоторезистивной маске на SEM-изображениях тестовых структур. Использование традиционных методов обнаружения дефектов требует значительных затрат времени и средств, так как эти методы основаны на ручном анализе изображений и требуют высокой квалификации специалистов. Данные методы имеют определённые ограничения, поскольку выполняемая классификация дефектов основана на заранее заложенных правилах. Эти ограничения часто приводят к неправильной классификации дефектов, тем самым увеличивая время анализа снимков. Кроме того, эти методы не позволяют обнаруживать и классифицировать новые типы дефектов, возникающих при переходе к более низким технологическим нормам. Возможное наличие шума на SEM-изображениях также негативно влияет на корректность результата детектирования. Использование нейросетевых детекторов позволяет автоматизировать процесс детекции дефектов при анализе больших наборов изображений, обеспечивает устойчивость к шуму и возможность обнаружения новых дефектных структур. Работа направлена на то, чтобы продемонстрировать возможность методов глубокого обучения точно классифицировать и локализовать различные типы дефектов фоторезистивной маски. 
В данной работе рассмотрены три популярные модели нейросетевых детекторов: `YoloV5`, `Faster R-CNN`, `RT-DETR`. Каждая из них имеет свои особенности и преимущества, которые будут подробно описаны в статье. Также будут приведены результаты экспериментов на собственном наборе данных, демонстрирующие эффективность каждой из моделей. Для оценки точности предсказаний использовались метрики **mAP@0.5** и **mAP@0.5-0.95**. Скорость детекторов оценивалась с помощью среднего числа кадров в секунду(**FPS**) во время инференса на тестовом наборе данных. 

## Датасет
Для создания датасета использовались изображения дефектов фоторезистивной маски в затворном слое для проектной нормы **28нм**.  Датасет содержит 500 RGB изображений формата “.tiff” размером **1024x1024 px**. При анализе имеющихся снимков удалось выделить 3 типа дефектных структур: **SRAF**(от англ. sub-resolution assist features) – вспомогательные непечатаемые структуры, добавляемые на кремниевую пластину для расширения окна процесса , **BRIDGE** – слияние двух соседних структур , **GAP** – промежуток в пределах одной структуры.
Пример каждого типа дефектных структур отражён на рисунке:


Каждый тип дефекта представлен 150 изображениями. Оставшиеся 50 изображений представляют собой структуры, не содержащие дефектов. Набор данных был разделён на тренировочную, валидационную и тестовую выборки в соотношении `76:17:7(380:83:37)`. В таблице 1 приведена дополнительная  статистика по датасету, включая количество изображений и количество структур для каждой из выборок. 

|        |Train|Validation|Test|
|:--------|:-----:|:----------:|:----:|
|  **GAP**   | 725 |	  85    |	53 |
| **SRAF**   |426  |	126     |	51 |
|**BRIDGE**  |348  |	79      |	45 |
|**Число структур** |	1499 |	290 |	149 |
|**Число изображений** |	380 |	83 |	37 |





## Эксперимент
В работе проведён сравнительный анализ моделей одностадийных (**YoloV5** и **RetinaNet**) и двустадийных детекторов(**FasterRCNN**). В силу небольшого размера датасета было решено использовать предобученные на датасете **MSCOCO** (*Microsoft* *Common* *Object* *in* *Context*) модели. Для обучения использовались 3 предобученные модели из семейства **YoloV5**: **YoloV5s**(*small*), **YoloV5m**(*medium*), **YoloV5(l)**(*large*) ,которые отличаются количеством параметров. Также отдельно был проведён эксперимент с заморозкой первых 15 слоёв у модели YoloV5m. У моделей Retina и FasterRCNN в качестве базовой сети для извлечения признаков использовалась предобученная модель **ResNet 50 FPN V2**.

* [Модель YoloV5](https://drive.google.com/file/d/1tWXDTCf9PU91_FQEolvEb5mwBZghlqMA/view?usp=sharing) ;
* [Модель RetinaNet](https://colab.research.google.com/drive/1RE86kC1hvKwxSVpbnZw43GGMJ4eaXm8P?usp=sharing);
* [Модель FasterRCNN](https://colab.research.google.com/drive/1Fp8zvTNBYwg4O5e6dEZuRF2ww6SJeckQ?usp=sharing).

## Отчёты по экспериментам
* [FasterRCNN](https://api.wandb.ai/links/ml_team_mskv/ct39z2bb)
* [RetinaNet](https://wandb.ai/ml_team_mskv/RetinaNet/reports/RetinaNet--Vmlldzo1MTEzOTE4?accessToken=lphq8w1ekatlghijp4tzxrlqjn0ipgtg73jyrz9199o5kzodobo71g85l5gi1h1l)
* [YoloV5](https://wandb.ai/ml_team_mskv/YOLOv5/reports/YoloV5--Vmlldzo1MTA2OTMw?accessToken=z32xznnqlqqtrt1i3xuqhu4bimcaeh0ys879jus351ugovp3un8lximih0e038kx)
