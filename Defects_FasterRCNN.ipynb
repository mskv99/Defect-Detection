{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6r0FlB9dErZ"
      },
      "source": [
        "# Downloading repositories and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN8pyc31Gswu"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mskv99/Defect-Detection.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHzm6MYZJG6k"
      },
      "outputs": [],
      "source": [
        "%cd /content/Defect-Detection/NetDirectory/\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYxSog6cNylE"
      },
      "outputs": [],
      "source": [
        "#uncomment to download from google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaAPc2wtOOVh"
      },
      "outputs": [],
      "source": [
        "#unzipping content from google drive\n",
        "#!unzip /content/drive/MyDrive/Networks/RetinaNet.zip -d /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIXlMYZYPAlp"
      },
      "outputs": [],
      "source": [
        "#!rm -rf /content/__MACOSX/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQuLjRnDS6X7"
      },
      "outputs": [],
      "source": [
        "# %cd /content/RetinaNet/\n",
        "# !pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5nHT4svSTMSr"
      },
      "outputs": [],
      "source": [
        "#first version of dataset\n",
        "#!curl -L \"https://app.roboflow.com/ds/UKv4tybmhd?key=cS4UxrytMH\" > roboflow.zip; unzip roboflow.zip -d custom_data; rm roboflow.zip\n",
        "\n",
        "#second version of dataset\n",
        "\n",
        "!curl -L \"https://app.roboflow.com/ds/EKzbJNgSDg?key=xJdZLnl59h\" > roboflow.zip; unzip roboflow.zip -d custom_data; rm roboflow.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbNEKnqLTQp7"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS47OuZaTXi7"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLLuEtfFTadc"
      },
      "outputs": [],
      "source": [
        "!wandb login\n",
        "\n",
        "#api key for weights and biases: 7f7117ef2660f827c823ba03863048fe0eea4801"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mit61ZsSTjqs"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"FasterRCNN\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.007475,\n",
        "    \"architecture\": \"ResNet50_fpn_v2\",\n",
        "    \"dataset\": \"Defect_dataset_v2\",\n",
        "    \"epochs\": 50,\n",
        "    \"optimizer\" : \"SGD\",\n",
        "    \"batch_size\" : \"8\",\n",
        "    \"augmentations\" : 'blur',\n",
        "    \"scheduler\" : 'step_lr_10_ep_gamma_0.1'\n",
        "    }\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dgtA4VKdSQB"
      },
      "source": [
        "# Creating dataset class and applying transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQrM3wD2UJBu"
      },
      "outputs": [],
      "source": [
        "#like a congif file\n",
        "import torch\n",
        "\n",
        "BATCH_SIZE = 8 # Increase / decrease according to GPU memeory.\n",
        "RESIZE_TO = 640 # Resize the image for training and transforms.\n",
        "NUM_EPOCHS = 50 # Number of epochs to train for.\n",
        "NUM_WORKERS = 2 # Number of parallel workers for data loading.\n",
        "LEARNING_RATE = 0.007475\n",
        "STEP = 10\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Training images and XML files directory.\n",
        "TRAIN_DIR = '/content/Defect-Detection/NetDirectory/custom_data/train'\n",
        "# Validation images and XML files directory.\n",
        "VALID_DIR = '/content/Defect-Detection/NetDirectory/custom_data/valid'\n",
        "\n",
        "# Classes: 0 index is reserved for background.\n",
        "# CLASSES = [\n",
        "#     '__background__', 'bridge', 'collapse', 'gap', 'sraf'\n",
        "# ]\n",
        "\n",
        "CLASSES = [\n",
        "    '__background__', 'bridge', 'collapse', 'gap', 'sraf'\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "# Whether to visualize images after crearing the data loaders.\n",
        "VISUALIZE_TRANSFORMED_IMAGES = False\n",
        "\n",
        "# Location to save model and plots.\n",
        "OUT_DIR = '/content/Defect-Detection/NetDirectory/outputs'\n",
        "\n",
        "\n",
        "# display_ids = {'__background__': 0, 'bridge' : 1, 'collapse': 2, 'gap' : 3,\n",
        "#                'non-bridge': 4, 'normal' : 5, 'sraf' : 6}\n",
        "\n",
        "# class_id_to_label = { int(v): k for k,v in display_ids.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt00uiNncWgJ"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    To handle the data loading as different images may have different number\n",
        "    of objects and to handle varying size tensors as well.\n",
        "    \"\"\"\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Define the training tranforms.\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.OneOf([\n",
        "            A.Blur(blur_limit=3, p=0.5),\n",
        "            A.MotionBlur(blur_limit=3, p=0.5),\n",
        "            A.MedianBlur(blur_limit=3, p=0.5),\n",
        "        ], p=0.5),\n",
        "        A.ToGray(p=0.1),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], bbox_params={\n",
        "        'format': 'pascal_voc',\n",
        "        'label_fields': ['labels']\n",
        "    })\n",
        "\n",
        "# Define the validation transforms.\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], bbox_params={\n",
        "        'format': 'pascal_voc',\n",
        "        'label_fields': ['labels']\n",
        "    })\n",
        "\n",
        "def log_bounding_boxes(image, v_boxes, v_labels, v_scores):\n",
        "  all_boxes = []\n",
        "  for b_i, box in enumerate(v_boxes):\n",
        "    # get coordiantes and labels\n",
        "    box_data = {\"position\" : {\n",
        "              \"minX\" : box.xmin,\n",
        "              \"maxX\" : box.xmax,\n",
        "              \"minY\" : box.ymin,\n",
        "              \"maxY\" : box.ymax},\n",
        "              \"class_id\" : display_ids[v_labels[(b_i).numpy().astype(np.int32)]],\n",
        "              # optionally caption each box with its class and score\n",
        "              \"box_caption\" : \"%s (%.3f)\" % (v_labels[b_i], v_scores[b_i]),\n",
        "              \"domain\" : \"pixel\",\n",
        "              \"scores\" : { \"score\" : v_scores[b_i] }}\n",
        "    all_boxes.append(box_data)\n",
        "    #log to wandb: image, predictions and dictionary of class labels for each class id\n",
        "    #box_image = wandb.Image(image, boxes = {\"predictions\": {\"box_data\"} })\n",
        "    box_image = wandb.Image(image, boxes = {\"predictions\": {\"box_data\": all_boxes, \"class_labels\" : class_id_to_label}})\n",
        "    wandb.log({\"bounding_boxes\": box_image})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r7BBTBlVRR9"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob as glob\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from xml.etree import ElementTree as et\n",
        "\n",
        "# from config import (\n",
        "#     CLASSES, RESIZE_TO, TRAIN_DIR, BATCH_SIZE, VALID_DIR,\n",
        "#     NUM_WORKERS, DEVICE, OUT_DIR, NUM_EPOCHS, NUM_CLASSES\n",
        "# )\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from custom_utils import collate_fn, get_train_transform, get_valid_transform\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "# The dataset class.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dir_path, width, height, classes, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.dir_path = dir_path\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.classes = classes\n",
        "        self.image_file_types = ['*.jpg', '*.jpeg', '*.png', '*.ppm', '*.JPG']\n",
        "        self.all_image_paths = []\n",
        "\n",
        "        # Get all the image paths in sorted order.\n",
        "        for file_type in self.image_file_types:\n",
        "            self.all_image_paths.extend(glob.glob(os.path.join(self.dir_path, file_type)))\n",
        "        self.all_images = [image_path.split(os.path.sep)[-1] for image_path in self.all_image_paths]\n",
        "        self.all_images = sorted(self.all_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Capture the image name and the full image path.\n",
        "        image_name = self.all_images[idx]\n",
        "\n",
        "        image_path = os.path.join(self.dir_path, image_name)\n",
        "\n",
        "        # Read and preprocess the image.\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image_resized = cv2.resize(image, (self.width, self.height))\n",
        "        #image_resized /= 255.0\n",
        "\n",
        "        # Capture the corresponding XML file for getting the annotations.\n",
        "        annot_filename = os.path.splitext(image_name)[0] + '.xml'\n",
        "        annot_file_path = os.path.join(self.dir_path, annot_filename)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        tree = et.parse(annot_file_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Original image width and height.\n",
        "        image_width = image.shape[1]\n",
        "        image_height = image.shape[0]\n",
        "\n",
        "        # Box coordinates for xml files are extracted\n",
        "        # and corrected for image size given.\n",
        "        for member in root.findall('object'):\n",
        "            # Get label and map the `classes`.\n",
        "            labels.append(self.classes.index(member.find('name').text))\n",
        "\n",
        "            # Left corner x-coordinates.\n",
        "            xmin = int(member.find('bndbox').find('xmin').text)\n",
        "            # Right corner x-coordinates.\n",
        "            xmax = int(member.find('bndbox').find('xmax').text)\n",
        "            # Left corner y-coordinates.\n",
        "            ymin = int(member.find('bndbox').find('ymin').text)\n",
        "            # Right corner y-coordinates.\n",
        "            ymax = int(member.find('bndbox').find('ymax').text)\n",
        "\n",
        "            # Resize the bounding boxes according\n",
        "            # to resized image `width`, `height`.\n",
        "            xmin_final = (xmin/image_width)*self.width\n",
        "            xmax_final = (xmax/image_width)*self.width\n",
        "            ymin_final = (ymin/image_height)*self.height\n",
        "            ymax_final = (ymax/image_height)*self.height\n",
        "\n",
        "            # Check that all coordinates are within the image.\n",
        "            if xmax_final > self.width:\n",
        "                xmax_final = self.width\n",
        "            if ymax_final > self.height:\n",
        "                ymax_final = self.height\n",
        "\n",
        "            boxes.append([xmin_final, ymin_final, xmax_final, ymax_final])\n",
        "\n",
        "        # Bounding box to tensor.\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # Area of the bounding boxes.\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 \\\n",
        "            else torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # No crowd instances.\n",
        "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "        # Labels to tensor.\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Prepare the final `target` dictionary.\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        image_id = torch.tensor([idx])\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "        # Apply the image transforms.\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image = image_resized,\n",
        "                                     bboxes = target['boxes'],\n",
        "                                     labels = labels)\n",
        "            image_resized = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "\n",
        "        if np.isnan((target['boxes']).numpy()).any() or target['boxes'].shape == torch.Size([0]):\n",
        "            target['boxes'] = torch.zeros((0, 4), dtype=torch.int64)\n",
        "        return image_resized, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "\n",
        "# Prepare the final datasets and data loaders.\n",
        "def create_train_dataset(DIR):\n",
        "    train_dataset = CustomDataset(\n",
        "        DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform()\n",
        "    )\n",
        "    return train_dataset\n",
        "def create_valid_dataset(DIR):\n",
        "    valid_dataset = CustomDataset(\n",
        "        DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform()\n",
        "    )\n",
        "    return valid_dataset\n",
        "def create_train_loader(train_dataset, num_workers=0, B_SIZE = BATCH_SIZE):\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = B_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers = num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=True\n",
        "    )\n",
        "    return train_loader\n",
        "def create_valid_loader(valid_dataset, num_workers=0, B_SIZE = BATCH_SIZE):\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size = B_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=True\n",
        "    )\n",
        "    return valid_loader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# sanity check of the Dataset pipeline with sample visualization\n",
        "dataset = CustomDataset(\n",
        "    TRAIN_DIR, RESIZE_TO, RESIZE_TO,CLASSES, transforms = None\n",
        ")\n",
        "print(f\"Number of training images: {len(dataset)}\")\n",
        "\n",
        "\n",
        "\n",
        "#colors = np.random.randint(0,255, size=(len(CLASSES), 3))\n",
        "colors = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
        "\n",
        "def visualize_sample(image, target):\n",
        "\n",
        "    pred_classes = [CLASSES[i] for i in target['labels']]\n",
        "    for box_num in range(len(target['boxes'])):\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        box = target['boxes'][box_num]\n",
        "        #label = CLASSES[target['labels'][box_num]]\n",
        "        class_name = pred_classes[box_num]\n",
        "        COLOR = colors[CLASSES.index(class_name)]\n",
        "\n",
        "\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
        "            color = COLOR[::-1],\n",
        "            thickness = 2\n",
        "        )\n",
        "        cv2.putText(\n",
        "            image,\n",
        "            class_name,\n",
        "            (int(box[0]), int(box[1]-5)),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.7,\n",
        "            color = COLOR[::-1],\n",
        "            thickness = 2\n",
        "        )\n",
        "        #print(colors[CLASSES.index(label)])\n",
        "    cv2_imshow(image)\n",
        "    print('\\n','\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju-8J7gyy_Gq"
      },
      "outputs": [],
      "source": [
        "def show_tranformed_image(train_loader, device, classes, colors):\n",
        "    \"\"\"\n",
        "    This function shows the transformed images from the `train_loader`.\n",
        "    Helps to check whether the tranformed images along with the corresponding\n",
        "    labels are correct or not.\n",
        "    \"\"\"\n",
        "    if len(train_loader) > 0:\n",
        "        for i in range(5):\n",
        "            images, targets = next(iter(train_loader))\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
        "            labels = targets[i]['labels'].cpu().numpy().astype(np.int32)\n",
        "            # Get all the predicited class names.\n",
        "            pred_classes = [classes[i] for i in targets[i]['labels'].cpu().numpy()]\n",
        "            sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "            sample = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            lw = max(round(sum(sample.shape) / 2 * 0.003), 2)  # Line width.\n",
        "            tf = max(lw - 1, 1) # Font thickness.\n",
        "\n",
        "            for box_num, box in enumerate(boxes):\n",
        "                p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "                class_name = pred_classes[box_num]\n",
        "                color = colors[classes.index(class_name)]\n",
        "                cv2.rectangle(\n",
        "                    sample,\n",
        "                    p1,\n",
        "                    p2,\n",
        "                    color,\n",
        "                    2,\n",
        "                    cv2.LINE_AA\n",
        "                )\n",
        "                w, h = cv2.getTextSize(\n",
        "                    class_name,\n",
        "                    0,\n",
        "                    fontScale=lw / 3,\n",
        "                    thickness=tf\n",
        "                )[0]  # text width, height\n",
        "                outside = p1[1] - h >= 3\n",
        "                p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
        "                cv2.putText(\n",
        "                    sample,\n",
        "                    class_name,\n",
        "                    (p1[0], p1[1] - 5 if outside else p1[1] + h + 2),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.8,\n",
        "                    color,\n",
        "                    2,\n",
        "                    cv2.LINE_AA\n",
        "                )\n",
        "            cv2_imshow(sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6SLqfskFVkLx"
      },
      "outputs": [],
      "source": [
        "random_numb = random.sample(range(0,len(dataset)), 5)\n",
        "print(random_numb)\n",
        "\n",
        "for _,j in enumerate(random_numb):\n",
        "  image, target = dataset[j]\n",
        "  visualize_sample(image, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEYsryXRYxH3"
      },
      "outputs": [],
      "source": [
        "train_dataset = create_train_dataset(TRAIN_DIR)\n",
        "train_loader = create_train_loader(train_dataset, NUM_WORKERS)\n",
        "\n",
        "valid_dataset = create_valid_dataset(VALID_DIR)\n",
        "valid_loader = create_valid_loader(valid_dataset, NUM_WORKERS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ncOIQhDhY0Ml"
      },
      "outputs": [],
      "source": [
        "show_tranformed_image(train_loader, DEVICE, CLASSES, colors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L_XO0lFdgBv"
      },
      "source": [
        "# Creating a model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNCu4DKidj84"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_model(num_classes = 5):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7BglIYVLZpL"
      },
      "source": [
        "# Training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1MDXdOSd66q"
      },
      "outputs": [],
      "source": [
        "from custom_utils import (\n",
        "    Averager,\n",
        "    SaveBestModel,\n",
        "    save_model,\n",
        "    save_loss_plot,\n",
        "    save_mAP\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Function for running training iterations.\n",
        "def train(train_data_loader, model):\n",
        "    print('Training')\n",
        "    model.train()\n",
        "\n",
        "     # initialize tqdm progress bar\n",
        "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
        "    cumm_loss = 0\n",
        "\n",
        "    for i, data in enumerate(prog_bar):\n",
        "        optimizer.zero_grad()\n",
        "        images, targets = data\n",
        "\n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "        cumm_loss += loss_value\n",
        "        #train_loss_hist.send(loss_value)\n",
        "\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update the loss value beside the progress bar for each iteration\n",
        "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
        "    return cumm_loss / len(train_data_loader)\n",
        "\n",
        "# Function for running validation iterations.\n",
        "def validate(valid_data_loader, model):\n",
        "    print('Validating')\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize tqdm progress bar.\n",
        "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
        "    target = []\n",
        "    preds = []\n",
        "    for i, data in enumerate(prog_bar):\n",
        "        images, targets = data\n",
        "\n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images, targets)\n",
        "\n",
        "        #losses = sum(loss for loss in outputs.values())\n",
        "        #val_loss_value = losses.item()\n",
        "\n",
        "        #val_loss_hist.send(val_loss_value)\n",
        "\n",
        "\n",
        "        # For mAP calculation using Torchmetrics.\n",
        "        #####################################\n",
        "        for i, image in enumerate(images):\n",
        "            true_dict = dict()\n",
        "            preds_dict = dict()\n",
        "            true_dict['boxes'] = targets[i]['boxes'].detach().cpu()\n",
        "            true_dict['labels'] = targets[i]['labels'].detach().cpu()\n",
        "            preds_dict['boxes'] = outputs[i]['boxes'].detach().cpu()\n",
        "            preds_dict['scores'] = outputs[i]['scores'].detach().cpu()\n",
        "            preds_dict['labels'] = outputs[i]['labels'].detach().cpu()\n",
        "            preds.append(preds_dict)\n",
        "            target.append(true_dict)\n",
        "\n",
        "            # log first six images from each batch\n",
        "            # if i < 6:\n",
        "            #   log_bounding_boxes(image, preds_dict['boxes'], preds_dict['labels'],\n",
        "            #                      preds_dict['scores'] )\n",
        "        #####################################\n",
        "\n",
        "\n",
        "\n",
        "    metric.reset()\n",
        "    metric.update(preds, target)\n",
        "    metric_summary = metric.compute()\n",
        "    return metric_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhXH4Fvz8j5A"
      },
      "outputs": [],
      "source": [
        "os.makedirs('outputs', exist_ok=True)\n",
        "# train_dataset = create_train_dataset(TRAIN_DIR)\n",
        "# valid_dataset = create_valid_dataset(VALID_DIR)\n",
        "# train_loader = create_train_loader(train_dataset, NUM_WORKERS)\n",
        "# valid_loader = create_valid_loader(valid_dataset, NUM_WORKERS)\n",
        "\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
        "\n",
        "# Initialize the model and move to the computation device.\n",
        "model = create_model(num_classes=NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "print(model)\n",
        "# Total parameters and trainable parameters.\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} training parameters.\")\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr = LEARNING_RATE, momentum=0.9, nesterov=True)\n",
        "#optimizer = torch.optim.Adam(params, lr=0.00001, weight_decay=0.0005)\n",
        "scheduler = StepLR(\n",
        "    optimizer=optimizer, step_size = STEP, gamma=0.1, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEmhf6VDe9oo"
      },
      "outputs": [],
      "source": [
        "# To monitor training loss\n",
        "#train_loss_hist = Averager()\n",
        "\n",
        "\n",
        "# To store training loss and mAP values.\n",
        "train_loss_list = []\n",
        "#val_loss_list = []\n",
        "map_50_list = []\n",
        "map_list = []\n",
        "\n",
        "# Mame to save the trained model with.\n",
        "SAVE_PATH = '/content/Defect-Detection/NetDirectory/outputs/best_model.pth'\n",
        "\n",
        "\n",
        "# To save best model.\n",
        "save_best_model = SaveBestModel()\n",
        "\n",
        "metric = MeanAveragePrecision(class_metrics=True)\n",
        "\n",
        "# Training loop.\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
        "\n",
        "    # Reset the training loss histories for the current epoch.\n",
        "    #train_loss_hist.reset()\n",
        "\n",
        "    # Start timer and carry out training and validation.\n",
        "    start = time.time()\n",
        "    train_loss = train(train_loader, model)\n",
        "    metric_summary = validate(valid_loader, model)\n",
        "\n",
        "    print(f\"Epoch #{epoch+1} train loss: {train_loss:.3f}\")\n",
        "\n",
        "    print(f\"Epoch #{epoch+1} mAP: {metric_summary['map']}\")\n",
        "    end = time.time()\n",
        "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "    map_50_list.append(metric_summary['map_50'])\n",
        "    map_list.append(metric_summary['map'])\n",
        "\n",
        "    wandb.log({'epoch': epoch,\n",
        "               'train_loss': round(train_loss, 2),\n",
        "               'map_50': metric_summary['map_50'],\n",
        "               'map': metric_summary['map']}\n",
        "      )\n",
        "\n",
        "\n",
        "    # save the best model till now.\n",
        "    save_best_model(\n",
        "        model, float(metric_summary['map']), epoch, 'outputs'\n",
        "    )\n",
        "    # Save the current epoch model.\n",
        "    save_model(epoch, model, optimizer)\n",
        "\n",
        "    # Save loss plot.\n",
        "    save_loss_plot(OUT_DIR, train_loss_list)\n",
        "\n",
        "    # Save mAP plot.\n",
        "    save_mAP(OUT_DIR, map_50_list, map_list)\n",
        "    scheduler.step()\n",
        "\n",
        "model_artifact = wandb.Artifact('FasterRCNN', type = 'model',\n",
        "                            description = 'FasterRCNN model with resnet_fpn backbone; blur augmentations'\n",
        "                            )\n",
        "model_artifact.add_file(SAVE_PATH)\n",
        "\n",
        "wandb.log_artifact(model_artifact)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxZ-zgyCasIP"
      },
      "outputs": [],
      "source": [
        "map_img = cv2.imread('/content/Defect-Detection/NetDirectory/outputs/map.png')\n",
        "loss_img = cv2.imread('/content/Defect-Detection/NetDirectory/outputs/train_loss.png')\n",
        "cv2_imshow(map_img)\n",
        "cv2_imshow(loss_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QubuO4bqVM_t"
      },
      "source": [
        "# Evaluating the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qVsIvkdUvZi"
      },
      "outputs": [],
      "source": [
        "#checkpoint = torch.load('outputs/best_model.pth', map_location=DEVICE)\n",
        "\n",
        "checkpoint = torch.load('/content/Defect-Detection/NetDirectory/outputs/best_model.pth', map_location=DEVICE)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(DEVICE).eval()\n",
        "\n",
        "metric = MeanAveragePrecision(class_metrics=True)\n",
        "metric_summary = validate(valid_loader, model)\n",
        "\n",
        "print(f\"mAP_50: {metric_summary['map_50']*100:.3f}\")\n",
        "print(f\"mAP_50_95: {metric_summary['map']*100:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuIFq20Ny-6f"
      },
      "outputs": [],
      "source": [
        "stats = validate(valid_loader, model)\n",
        "\n",
        "print('\\n')\n",
        "print(stats)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"Classes: {CLASSES}\")\n",
        "print('\\n')\n",
        "print('AP / AR per class')\n",
        "empty_string = ''\n",
        "if len(CLASSES) > 2:\n",
        "    num_hyphens = 73\n",
        "    print('-'*num_hyphens)\n",
        "    print(f\"|    | Class{empty_string:<16}| AP{empty_string:<18}| AR{empty_string:<18}|\")\n",
        "    print('-'*num_hyphens)\n",
        "    class_counter = 0\n",
        "    for i in range(0, len(CLASSES)-1, 1):\n",
        "        class_counter += 1\n",
        "        print(f\"|{class_counter:<3} | {CLASSES[i+1]:<20} | {np.array(stats['map_per_class'][i]):.3f}{empty_string:<15}| {np.array(stats['mar_100_per_class'][i]):.3f}{empty_string:<15}|\")\n",
        "    print('-'*num_hyphens)\n",
        "    print(f\"|Avg{empty_string:<23} | {np.array(stats['map']):.3f}{empty_string:<15}| {np.array(stats['mar_100']):.3f}{empty_string:<15}|\")\n",
        "else:\n",
        "    num_hyphens = 62\n",
        "    print('-'*num_hyphens)\n",
        "    print(f\"|Class{empty_string:<10} | AP{empty_string:<18}| AR{empty_string:<18}|\")\n",
        "    print('-'*num_hyphens)\n",
        "    print(f\"|{CLASSES[1]:<15} | {np.array(stats['map']):.3f}{empty_string:<15}| {np.array(stats['mar_100']):.3f}{empty_string:<15}|\")\n",
        "    print('-'*num_hyphens)\n",
        "    print(f\"|Avg{empty_string:<12} | {np.array(stats['map']):.3f}{empty_string:<15}| {np.array(stats['mar_100']):.3f}{empty_string:<15}|\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP6tz1jmW2lk"
      },
      "source": [
        "# Model inference on test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppp1ic5TWqgQ"
      },
      "outputs": [],
      "source": [
        "os.makedirs('inference_outputs/images', exist_ok=True)\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
        "\n",
        "DIR_TEST = '/content/Defect-Detection/NetDirectory/custom_data/test'\n",
        "test_images = glob.glob(f\"{DIR_TEST}/*.jpg\")\n",
        "print(f\"Test instances: {len(test_images)}\")\n",
        "\n",
        "TRESHOLD_VALUE = 0.5\n",
        "\n",
        "\n",
        "\n",
        "frame_count = 0 # To count total frames.\n",
        "total_fps = 0 # To get the final frames per second.\n",
        "\n",
        "for i in range(len(test_images)):\n",
        "    # Get the image file name for saving output later on.\n",
        "    image_name = test_images[i].split(os.path.sep)[-1].split('.')[0]\n",
        "    image = cv2.imread(test_images[i])\n",
        "    orig_image = image.copy()\n",
        "\n",
        "    print(image.shape)\n",
        "    # BGR to RGB.\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    # Make the pixel range between 0 and 1.\n",
        "    #image /= 255.0\n",
        "    # Bring color channels to front (H, W, C) => (C, H, W).\n",
        "    image_input = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "    # Convert to tensor.\n",
        "    image_input = torch.tensor(image_input, dtype=torch.float).cuda()\n",
        "    # Add batch dimension.\n",
        "    image_input = torch.unsqueeze(image_input, 0)\n",
        "    start_time = time.time()\n",
        "    # Predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_input.to(DEVICE))\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Get the current fps.\n",
        "    fps = 1 / (end_time - start_time)\n",
        "    # Total FPS till current frame.\n",
        "    total_fps += fps\n",
        "    frame_count += 1\n",
        "\n",
        "    # Load all detection to CPU for further operations.\n",
        "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
        "    # Carry further only if there are detected boxes.\n",
        "    if len(outputs[0]['boxes']) != 0:\n",
        "        boxes = outputs[0]['boxes'].data.numpy()\n",
        "        scores = outputs[0]['scores'].data.numpy()\n",
        "        # Filter out boxes according to `detection_threshold`.\n",
        "        boxes = boxes[scores >= TRESHOLD_VALUE].astype(np.int32)\n",
        "        draw_boxes = boxes.copy()\n",
        "        # Get all the predicited class names.\n",
        "        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
        "\n",
        "        # Draw the bounding boxes and write the class name on top of it.\n",
        "        for j, box in enumerate(draw_boxes):\n",
        "            class_name = pred_classes[j]\n",
        "            color = COLORS[CLASSES.index(class_name)]\n",
        "            # Recale boxes.\n",
        "            xmin = int((box[0] / image.shape[1]) * orig_image.shape[1])\n",
        "            ymin = int((box[1] / image.shape[0]) * orig_image.shape[0])\n",
        "            xmax = int((box[2] / image.shape[1]) * orig_image.shape[1])\n",
        "            ymax = int((box[3] / image.shape[0]) * orig_image.shape[0])\n",
        "            cv2.rectangle(orig_image,\n",
        "                        (xmin, ymin),\n",
        "                        (xmax, ymax),\n",
        "                        color[::-1],\n",
        "                        3)\n",
        "            cv2.putText(orig_image,\n",
        "                        class_name,\n",
        "                        (xmin, ymin-5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.8,\n",
        "                        color[::-1],\n",
        "                        2,\n",
        "                        lineType=cv2.LINE_AA)\n",
        "\n",
        "        cv2_imshow(orig_image)\n",
        "        cv2.imwrite(f\"inference_outputs/images/{image_name}.jpg\", orig_image)\n",
        "    print(f\"Image {i+1} done...\")\n",
        "    print('-'*50)\n",
        "\n",
        "print('TEST PREDICTIONS COMPLETE')\n",
        "# Calculate and print the average FPS.\n",
        "avg_fps = total_fps / frame_count\n",
        "print(f\"Average FPS: {avg_fps:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/FasterRCNN_inference_images.zip /content/Defect-Detection/NetDirectory/inference_outputs/images"
      ],
      "metadata": {
        "id": "lgmlQHlOznaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/FasterRCNN_inference_images.zip')"
      ],
      "metadata": {
        "id": "JzjkEMYoz-3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E37LRL12Cn6i"
      },
      "source": [
        "# Tuning hyperparameters for second dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HCmu6fFDMKD"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "        'name': 'train_loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'batch_size' : {\n",
        "            'values' : [6,7, 8, 9, 10]\n",
        "\n",
        "        'optimizer' : {\n",
        "            'values' : ['SGD','Adam']\n",
        "        },\n",
        "        'learning_rate': {'distribution': 'uniform',\n",
        "                          'max': 0.01,\n",
        "                          'min': 0.00001},\n",
        "        'epochs' : {'value': 5}\n",
        "    },\n",
        "    'description' : 'tuning hyperparameters for first dataset'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8kqZiGpgXiS"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)\n",
        "os.makedirs('outputs', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h39999JgePx"
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project = 'FasterRCNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04u5aaPerRf-"
      },
      "outputs": [],
      "source": [
        "from custom_utils import (\n",
        "    Averager,\n",
        "    SaveBestModel,\n",
        "    save_model,\n",
        "    save_loss_plot,\n",
        "    save_mAP\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(train_data_loader, model, optimizer):\n",
        "    print('Training')\n",
        "    model.train()\n",
        "\n",
        "\n",
        "     # initialize tqdm progress bar\n",
        "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
        "    cumm_loss = 0\n",
        "\n",
        "    for i, data in enumerate(prog_bar):\n",
        "        optimizer.zero_grad()\n",
        "        images, targets = data\n",
        "\n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "        cumm_loss += loss_value\n",
        "\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update the loss value beside the progress bar for each iteration\n",
        "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
        "\n",
        "    return cumm_loss/len(train_data_loader)\n",
        "\n",
        "    #return loss_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7kg2CEngqIC"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "train_loss_hist = Averager()\n",
        "\n",
        "def tune_param(config=None):\n",
        "  #global train_loss_hist\n",
        "  wandb.init(project = 'FasterRCNN', config = config)\n",
        "  config = wandb.config\n",
        "  train_dataset = create_train_dataset(TRAIN_DIR)\n",
        "  train_loader = create_train_loader(train_dataset, num_workers = 2, B_SIZE = config.batch_size)\n",
        "\n",
        "  model = create_model(num_classes = NUM_CLASSES)\n",
        "  model = model.to(DEVICE)\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "  # optimizer = torch.optim.SGD(params, lr=config.learning_rate)\n",
        "  # print(optimizer)\n",
        "\n",
        "  if config.optimizer == 'SGD':\n",
        "    optimizer = torch.optim.SGD(params, lr = config.learning_rate, momentum = 0.9)\n",
        "  elif config.optimizer == 'Adam':\n",
        "    optimizer = torch.optim.Adam(params, lr = config.learning_rate)\n",
        "  wandb.watch(model, log= 'all')\n",
        "\n",
        "\n",
        "  for epoch in range(config.epochs):\n",
        "      print(f\"\\nEPOCH {epoch+1} of {config.epochs}\")\n",
        "\n",
        "      # Reset the training loss histories for the current epoch.\n",
        "      #train_loss_hist.reset()\n",
        "\n",
        "      # Start timer and carry out training and validation.\n",
        "      start = time.time()\n",
        "      train_loss_epoch = train(train_loader, model, optimizer = optimizer)\n",
        "      print(f\"Epoch #{epoch+1} train loss: {train_loss_epoch:.3f}\")\n",
        "\n",
        "      end = time.time()\n",
        "      print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch+1}\")\n",
        "\n",
        "\n",
        "      wandb.log({'epoch': epoch,\n",
        "                'train_loss': round(train_loss_epoch, 2)\n",
        "                })\n",
        "\n",
        "  wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTLiDyFwkwf5"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, function = tune_param, count = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dKbXPXb0ElQ"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('writing a simple message to test pushing from google colab')"
      ],
      "metadata": {
        "id": "tf6gOXh2qdCv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "E37LRL12Cn6i"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}